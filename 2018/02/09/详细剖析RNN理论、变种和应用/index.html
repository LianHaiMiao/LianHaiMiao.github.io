<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="RNN,LSTM,Language Model," />










<meta name="description" content="详细剖析RNN理论、变种和应用">
<meta name="keywords" content="RNN,LSTM,Language Model">
<meta property="og:type" content="article">
<meta property="og:title" content="详细剖析RNN理论、变种和应用">
<meta property="og:url" content="http://yoursite.com/2018/02/09/详细剖析RNN理论、变种和应用/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="详细剖析RNN理论、变种和应用">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/images/RNN/RNN-rolled.jpg">
<meta property="og:image" content="http://yoursite.com/images/RNN/RNN.jpg">
<meta property="og:image" content="http://yoursite.com/images/RNN/one2many.png">
<meta property="og:image" content="http://yoursite.com/images/RNN/many2one.png">
<meta property="og:image" content="http://yoursite.com/images/RNN/many2many.png">
<meta property="og:image" content="http://yoursite.com/images/RNN/encoder-decoder.png">
<meta property="og:image" content="http://yoursite.com/images/RNN/rnn-bptt.svg">
<meta property="og:updated_time" content="2018-04-10T12:21:28.080Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="详细剖析RNN理论、变种和应用">
<meta name="twitter:description" content="详细剖析RNN理论、变种和应用">
<meta name="twitter:image" content="http://yoursite.com/images/RNN/RNN-rolled.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/02/09/详细剖析RNN理论、变种和应用/"/>





  <title>详细剖析RNN理论、变种和应用 | Hexo</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/09/详细剖析RNN理论、变种和应用/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lianhai Miao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">详细剖析RNN理论、变种和应用</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-09T17:20:13+08:00">
                2018-02-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/RNN/" itemprop="url" rel="index">
                    <span itemprop="name">RNN</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="详细剖析RNN理论、变种和应用"><a href="#详细剖析RNN理论、变种和应用" class="headerlink" title="详细剖析RNN理论、变种和应用"></a>详细剖析RNN理论、变种和应用</h1><a id="more"></a>
<h2 id="一、RNN-循环神经网络"><a href="#一、RNN-循环神经网络" class="headerlink" title="一、RNN(循环神经网络)"></a>一、RNN(循环神经网络)</h2><h3 id="1-1-RNN的原理"><a href="#1-1-RNN的原理" class="headerlink" title="1.1 RNN的原理"></a>1.1 RNN的原理</h3><p>以前我们写过的 Network 的结果都是反馈给下一层。</p>
<p>那在RNN中，我们不仅要输出结果，还要输出一个<strong>隐含状态</strong>，给当前层在处理下一个样本时使用。</p>
<p>结构图如下所示：</p>
<p><img src="/images/RNN/RNN-rolled.jpg" alt="Recurrent Neural Networks have loops"></p>
<p>可以看到，跟普通的 Neural Network 相比，我们这里仅仅是多了一个<strong>隐含状态</strong>，那么这个多出来的<strong>隐含状态</strong>有什么好处呢？</p>
<p>这个<strong>隐含状态</strong>，我们可以看作是上一个样本遗留给我们的信息，通过这个信息，我们可以把握过去样本的特征，结合当前的样本，组成一种类似于 “时间序列” 样本信息，从而让模型有了处理前后有依赖关系数据样本的能力。</p>
<p><strong>举个例子:</strong></p>
<p>语言模型的任务是给定句子的前 $t$ 个字符，然后预测第 $t+1$ 个字符。假设我们的句子是“你好世界”，使用循环神经网络来预测的一个做法是，在时间 $1$ 输入“你”，预测“好”同时生成一个<strong>隐含状态</strong>；</p>
<p>在时间 $2$ 向同一个网络输入刚才生成的<strong>隐含状态</strong> 和 “好” 去预测“世”。同时输出一个更新过的隐含状态。</p>
<p>对于这个<strong>隐含状态</strong>， 我们会希望前面的信息能够保存在这个隐含状态里，从而提升预测效果。下图展示了这个过程。</p>
<p><img src="/images/RNN/RNN.jpg" alt="Recurrent Neural Networks have loops"></p>
<h3 id="1-2-RNN的公式表示"><a href="#1-2-RNN的公式表示" class="headerlink" title="1.2 RNN的公式表示"></a>1.2 RNN的公式表示</h3><p>针对输入输出加上时间状态 $t$。在 $t$ 时刻，我们的输入为 $X_t$ 和 $H_{t-1}$，经过一个隐含层之后，我们有输出 $H_t$，经过最后的输出层得到 $\hat{Y_t}$。</p>
<p>其中 $X_t \in R^{n \times x}$ (样本数量为 $n$，每个样本特征向量维度是 $x$) ； $H_{t-1} \in R^{n \times h}$ （其中隐含层长度为 $h$）</p>
<p>隐含层的计算公式为：</p>
<p>$$\mathbf{H}_t = \phi(\mathbf{X}<em>t \mathbf{W}</em>{xh} + \mathbf{H}<em>{t-1} \mathbf{W}</em>{hh}  + \mathbf{b}_h)$$</p>
<p>其中 $\mathbf{W}<em>{xh}$ 和 $\mathbf{W}</em>{hh}$ 是隐含层的权重， $\mathbf{b}_h$ 是隐含层的 bias。</p>
<p>输出层的计算公式为：</p>
<p>$$\hat{\mathbf{Y}}_t = \text{softmax}(\mathbf{H}<em>t \mathbf{W}</em>{hy}  + \mathbf{b}_y)$$</p>
<p>其中 $\mathbf{W}_{hy}$ 是输出层权重，$\mathbf{b}_y$ 是输出层的 bias</p>
<p>最开始的隐含状态里的元素通常会被初始化为0，也就是 $H_0$ 是个零矩阵。</p>
<h3 id="1-3-RNN的变种"><a href="#1-3-RNN的变种" class="headerlink" title="1.3 RNN的变种"></a>1.3 RNN的变种</h3><h4 id="1-3-1-one-to-many"><a href="#1-3-1-one-to-many" class="headerlink" title="1.3.1 one to many:"></a>1.3.1 one to many:</h4><p><img src="/images/RNN/one2many.png" alt="one to many"></p>
<p><strong>Music generation</strong> 、 <strong>poem generation</strong>: 我们给定起始的几个单词或者音符，自动为我们将剩下的内容补齐，从而创作出一首音乐或者一首诗词。这个时候，我们就可以 one to many 使用来解决这类问题。</p>
<h4 id="1-3-2-many-to-one"><a href="#1-3-2-many-to-one" class="headerlink" title="1.3.2 many to one:"></a>1.3.2 many to one:</h4><p><img src="/images/RNN/many2one.png" alt="many to one"></p>
<p><strong>Setiment Classfication</strong>：比如，给定我们一个电影评论，我们来判断这个电影评论的情感好坏，用 1-5 的分数来表示用户对电影的喜好程度。 这个时候，我们就可以使用 many to one 来解决这类问题。</p>
<h4 id="1-3-3-many-to-many-with-the-same-sequence"><a href="#1-3-3-many-to-many-with-the-same-sequence" class="headerlink" title="1.3.3 many to many with the same sequence:"></a>1.3.3 many to many with the same sequence:</h4><p><img src="/images/RNN/many2many.png" alt="many to many with the same length sequence"></p>
<p><strong>Name entity recognition</strong>： 给定我们一句话 Yesterday Harry Potter met Hermione Granger 我们判断其中的那些单词是姓名。 对应的输出应该是 0 1 1 0 1 1。这个时候，我们就可以使用 many to many with the same sequence 来解决这类问题。</p>
<h4 id="1-3-4-many-to-many-with-the-different-length-sequence-encoder-decoder"><a href="#1-3-4-many-to-many-with-the-different-length-sequence-encoder-decoder" class="headerlink" title="1.3.4 many to many with the different length sequence (encoder-decoder)"></a>1.3.4 many to many with the different length sequence (encoder-decoder)</h4><p><img src="/images/RNN/encoder-decoder.png" alt="encoder-decoder"></p>
<p><strong>Machine translation</strong>：英语和中文的翻译，通常情况下，英语用十几个词表示的句子，中文几个词就够了。那么这个时候，我们就可以使用 encoder-decoder 来解决这类问题。</p>
<h3 id="1-4-RNN的缺陷"><a href="#1-4-RNN的缺陷" class="headerlink" title="1.4 RNN的缺陷"></a>1.4 RNN的缺陷</h3><h4 id="1-4-1-时间反向传播（BPTT）-和-梯度爆炸（explosion）-和-梯度消失（valishing）"><a href="#1-4-1-时间反向传播（BPTT）-和-梯度爆炸（explosion）-和-梯度消失（valishing）" class="headerlink" title="1.4.1 时间反向传播（BPTT） 和 梯度爆炸（explosion） 和 梯度消失（valishing）"></a>1.4.1 时间反向传播（BPTT） 和 梯度爆炸（explosion） 和 梯度消失（valishing）</h4><p>为什么会出现 explosion 或 valishing 呢？</p>
<p>我们需要先介绍循环神经网络中模型梯度的计算和存储，也即通过<strong>时间反向传播</strong>（back-propagation through time）简称 <strong>BPTT</strong></p>
<p>事实上，所谓通过时间反向传播只是反向传播在循环神经网络的具体应用。我们只需将循环神经网络按时间展开，从而得到模型变量和参数之间的依赖关系，并依据链式法则应用反向传播计算梯度。</p>
<p>为了方便演示，我们先来看一个例子。</p>
<p>给定一个输入为 $\mathbf{x}_t \in \mathbb{R}^x$ （每个样本输入向量长度为 $x$） 和对应真实值为 $y_t \in \mathbb{R}$ 的时许数据训练样本（$t = 1, 2, \ldots, T$ 为时刻），为了简单，我们不考虑偏差 $b$， 那么隐含层的表达式可以写成：</p>
<p>$$\mathbf{h}<em>t = \phi(\mathbf{W}</em>{hx} \mathbf{x}<em>t + \mathbf{W}</em>{hh} \mathbf{h}_{t-1})$$</p>
<p>$\mathbf{W}<em>{hx} \in \mathbb{R}^{h \times x}$ 和 $\mathbf{W}</em>{hh} \in \mathbb{R}^{h \times h}$ 是隐含层模型参数。</p>
<p>$\mathbf{h}_t \in \mathbb{R}^h$ 是 $t$ 时刻，向量长度为 $h$ 的隐含层变量，</p>
<p>同时，我们考虑相应时刻的输出层变量 $\mathbf{o}_t \in \mathbb{R}^y$ ，为了简单，我们不考虑偏差项。</p>
<p>$$\mathbf{o}<em>t = \mathbf{W}</em>{yh} \mathbf{h}_{t}$$</p>
<p>其中 $\mathbf{W}_{yh} \in \mathbb{R}^{y \times h}$ 表示输出层模型参数。</p>
<p>给定每个时刻损失函数计算公式 $\ell$ ，长度为 $T$ 的整个时序数据的损失函数 $L$ 定义为:</p>
<p>$$L = \frac{1}{T} \sum_{t=1}^T \ell (\mathbf{o}_t, y_t)$$</p>
<p>这也是模型最终需要被优化的目标函数。</p>
<p>为了可视化模型变量和参数之间在计算中的依赖关系，我们可以绘制<strong>计算图</strong>。我们以时序长度 $T=3$ 为例。</p>
<p><img src="/images/RNN/rnn-bptt.svg" alt="rnn-bptt"></p>
<p>PS：图片来自 <a href="https://zh.gluon.ai/chapter_recurrent-neural-networks/bptt.html" target="_blank" rel="noopener">李沐gluon教程——通过时间反向传播</a></p>
<p>在上图中，模型的参数是$\mathbf{W}<em>{hx}$、$\mathbf{W}</em>{hh}$和$\mathbf{W}_{yh}$。为了在模型训练中学习这三个参数，以随机梯度下降为例，假设学习率为$η$，我们可以通过</p>
<p>$$\mathbf{W}<em>{hx} = \mathbf{W}</em>{hx} - \eta \frac{\partial L}{\partial \mathbf{W}_{hx}}$$</p>
<p>$$\mathbf{W}<em>{hh} = \mathbf{W}</em>{hh} - \eta \frac{\partial L}{\partial \mathbf{W}_{hh}}$$</p>
<p>$$\mathbf{W}<em>{yh} = \mathbf{W}</em>{yh} - \eta \frac{\partial L}{\partial \mathbf{W}_{yh}}$$</p>
<p>来不断迭代模型参数的值。因此我们需要模型参数梯度 $\partial L/\partial \mathbf{W}<em>{hx}$、$\partial L/\partial \mathbf{W}</em>{hh}$、$\partial L/\partial \mathbf{W}_{yh}$</p>
<p>为此，我们可以按照反向传播的次序依次计算并存储梯度。</p>
<p>为了表述方便，对输入输出 $\mathsf{X}, \mathsf{Y}, \mathsf{Z}$ 为任意形状张量的函数 $\mathsf{Y}=f(\mathsf{X})$ 和 $\mathsf{Z}=g(\mathsf{Y})$ ，我们使用</p>
<p>$$\frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \text{prod}(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}})$$</p>
<p>来表达链式法则。</p>
<p>回到刚才的模型中，我们首先计算出目标函数有关各时刻输出层变量的梯度 $\partial L/\partial \mathbf{o}_t \in \mathbb{R}^y$ 可以很容易地计算</p>
<p>$$\frac{\partial L}{\partial \mathbf{o}_t} =  \frac{\partial \ell (\mathbf{o}_t, y_t)}{T \cdot \partial \mathbf{o}_t}$$</p>
<p>事实上，这时我们已经可以计算目标函数有关模型参数 $\mathbf{W}<em>{yh}$ 的梯度 $\partial L/\partial \mathbf{W}</em>{yh} \in \mathbb{R}^{y \times h}$ 。需要注意的是，在计算图中，<strong>$\mathbf{W}_{yh}$ 可以经过 $\mathbf{o}_1, \ldots, \mathbf{o}_T$ 通向 $L$</strong> ，依据链式法则，</p>
<p>$$\frac{\partial L}{\partial \mathbf{W}<em>{yh}}= \sum</em>{t=1}^T \text{prod}(\frac{\partial L}{\partial \mathbf{o}_t}, \frac{\partial \mathbf{o}<em>t}{\partial \mathbf{W}</em>{yh}})= \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{o}_t} \mathbf{h}_t^\top$$</p>
<p>其次，我们注意到隐含层变量之间也有依赖关系。 对于最终时刻 $T$ ， 在计算图中， 隐含层变量 $\mathbf{h}_T$ 只经过 $\mathbf{o}_T$ 通向 $L$。因此我们先计算目标函数有关最终时刻隐含层变量的梯度 $\partial L/\partial \mathbf{h}_T \in \mathbb{R}^h$ 。依据链式法则，我们得到</p>
<p>$$\frac{\partial L}{\partial \mathbf{h}_T} = \text{prod}(\frac{\partial L}{\partial \mathbf{o}_T}, \frac{\partial \mathbf{o}_T}{\partial \mathbf{h}<em>T} ) = \mathbf{W}</em>{yh}^\top \frac{\partial L}{\partial \mathbf{o}_T}$$</p>
<p>为了简化计算，我们假设激活函数 $\phi(x) = x$ 。 接下来，对于时刻 $t &lt; T$， 在计算图中， 由于 $\mathbf{h}<em>t$ 可以经过 $\mathbf{h}</em>{t+1}$ 和 $\mathbf{o}_t$ 通向 $L$ ，依据链式法则， 目标函数有关隐含层变量的梯度 $\partial L/\partial \mathbf{h}_t \in \mathbb{R}^h$ 需要按照时刻从晚到早依次计算：</p>
<p>$$\frac{\partial L}{\partial \mathbf{h}<em>t} = \text{prod}(\frac{\partial L}{\partial \mathbf{h}</em>{t+1}}, \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_t} ) + \text{prod}(\frac{\partial L}{\partial \mathbf{o}_t}, \frac{\partial \mathbf{o}_t}{\partial \mathbf{h}<em>t} ) = \mathbf{W}</em>{hh}^\top \frac{\partial L}{\partial \mathbf{h}<em>{t+1}} + \mathbf{W}</em>{yh}^\top \frac{\partial L}{\partial \mathbf{o}_t}$$</p>
<p>将递归公式展开，对任意$1 \leq t \leq T$，我们可以得到目标函数有关隐含层变量梯度的通项公式</p>
<p>$$\frac{\partial L}{\partial \mathbf{h}<em>t} = \sum</em>{i=t}^T {(\mathbf{W}<em>{hh}^\top)}^{T-i} \mathbf{W}</em>{yh}^\top \frac{\partial L}{\partial \mathbf{o}_{T+t-i}}$$</p>
<p>由此可见，<strong>当每个时序训练数据样本的时序长度 $T$ 较大或者时刻 $t$ 较小</strong>，目标函数有关隐含层变量梯度较容易出现 <strong>衰减（valishing）</strong>和 <strong>爆炸（explosion）</strong>。想象一下$2^{30}$和$0.5^{30}$会有多大。</p>
<p>有了各时刻隐含层变量的梯度之后，我们可以计算隐含层中模型参数的梯度 $\partial L/\partial \mathbf{W}<em>{hx} \in \mathbb{R}^{h \times x}$ 和 $\partial L/\partial \mathbf{W}</em>{hh} \in \mathbb{R}^{h \times h}$ 。在计算图中，它们都可以经过 $\mathbf{h}_1, \ldots, \mathbf{h}_T$ 通向$L$。依据链式法则，我们有</p>
<p>$$\frac{\partial L}{\partial \mathbf{W}<em>{hx}} = \sum</em>{t=1}^T \text{prod}(\frac{\partial L}{\partial \mathbf{h}_t}, \frac{\partial \mathbf{h}<em>t}{\partial \mathbf{W}</em>{hx}}) = \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{h}_t} \mathbf{x}_t^\top$$</p>
<p>$$\frac{\partial L}{\partial \mathbf{W}<em>{hh}} = \sum</em>{t=1}^T \text{prod}(\frac{\partial L}{\partial \mathbf{h}_t}, \frac{\partial \mathbf{h}<em>t}{\partial \mathbf{W}</em>{hh}}) = \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{h}<em>t} \mathbf{h}</em>{t-1}^\top$$</p>
<h4 id="1-4-2-只考虑到了前序信息，后序信息没有考虑"><a href="#1-4-2-只考虑到了前序信息，后序信息没有考虑" class="headerlink" title="1.4.2 只考虑到了前序信息，后序信息没有考虑"></a>1.4.2 只考虑到了前序信息，后序信息没有考虑</h4><h2 id="二、GRU"><a href="#二、GRU" class="headerlink" title="二、GRU"></a>二、GRU</h2><h2 id="三、LSTM"><a href="#三、LSTM" class="headerlink" title="三、LSTM"></a>三、LSTM</h2><h2 id="四、参考"><a href="#四、参考" class="headerlink" title="四、参考"></a>四、参考</h2><p>(1) <a href="https://www.coursera.org/learn/nlp-sequence-models" target="_blank" rel="noopener">吴恩达 Deep Learning 系列课程第五课</a></p>
<p>(2) <a href="https://zh.gluon.ai/chapter_recurrent-neural-networks/rnn-scratch.html" target="_blank" rel="noopener">李沐gluon学习教程</a></p>
<p>(3) <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks</a></p>
<p>(4) <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/RNN/" rel="tag"># RNN</a>
          
            <a href="/tags/LSTM/" rel="tag"># LSTM</a>
          
            <a href="/tags/Language-Model/" rel="tag"># Language Model</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/01/07/推荐系统方法小结-下/" rel="next" title="推荐系统方法小结(下)">
                <i class="fa fa-chevron-left"></i> 推荐系统方法小结(下)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/02/27/ubuntu换源的那些事/" rel="prev" title="ubuntu换源的那些事">
                ubuntu换源的那些事 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Lianhai Miao</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">35</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">37</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#详细剖析RNN理论、变种和应用"><span class="nav-number">1.</span> <span class="nav-text">详细剖析RNN理论、变种和应用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、RNN-循环神经网络"><span class="nav-number">1.1.</span> <span class="nav-text">一、RNN(循环神经网络)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-RNN的原理"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1 RNN的原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-RNN的公式表示"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2 RNN的公式表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-RNN的变种"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3 RNN的变种</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-one-to-many"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">1.3.1 one to many:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-2-many-to-one"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">1.3.2 many to one:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-3-many-to-many-with-the-same-sequence"><span class="nav-number">1.1.3.3.</span> <span class="nav-text">1.3.3 many to many with the same sequence:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-4-many-to-many-with-the-different-length-sequence-encoder-decoder"><span class="nav-number">1.1.3.4.</span> <span class="nav-text">1.3.4 many to many with the different length sequence (encoder-decoder)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-RNN的缺陷"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.4 RNN的缺陷</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-1-时间反向传播（BPTT）-和-梯度爆炸（explosion）-和-梯度消失（valishing）"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">1.4.1 时间反向传播（BPTT） 和 梯度爆炸（explosion） 和 梯度消失（valishing）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-2-只考虑到了前序信息，后序信息没有考虑"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">1.4.2 只考虑到了前序信息，后序信息没有考虑</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、GRU"><span class="nav-number">1.2.</span> <span class="nav-text">二、GRU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三、LSTM"><span class="nav-number">1.3.</span> <span class="nav-text">三、LSTM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四、参考"><span class="nav-number">1.4.</span> <span class="nav-text">四、参考</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lianhai Miao</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
